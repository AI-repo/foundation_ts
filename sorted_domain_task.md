# A Survey of Foundation Models for Time-series in the Age of LLMs

### Table 1. Application Domains and Usage of Foundation Models
| **Domain**                | **Typical Time-Series Tasks** | **Foundation-Model Applications**  | **Representative Works**  |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Healthcare & Biomed**   | • Patient monitoring (forecast vitals, detect deterioration)  <br>• Diagnosis (EEG/ECG classification)  <br>• Anomaly detection (arrhythmia, seizures)  <br>• Medical data fusion (multi-sensor, multimodal)     | • General FMs pre-trained on large physiological databases  <br>• Domain-specific FMs (EEG, ECG) for efficient diagnosis  <br>• LLMs for clinical time-series reasoning with explanations                                                                   | *EEG-GPT* – GPT-based EEG interpretation  <br>*HeartLang* – ECG language model for arrhythmia detection  <br>*BFM* – brain-signal FM for EEG/fMRI generation                                                                |
| **Finance & Economics**   | • Forecasting (prices, demand)  <br>• Anomaly detection (fraud, market shifts)  <br>• Portfolio optimization (scenario generation)  <br>• Economic-indicator modeling                                           | • LLMs prompted with financial TS + news  <br>• Pre-trained models on market data for zero-shot forecasting  <br>• Multimodal models combining text (news) + time series for decision support                                                               | *Time-LLM* – applied to stock-trend prediction  <br>*LLM4TS* – economic-data forecasting  <br>*BloombergGPT* (50 B) – mixed financial text & time-series (2023)                                                             |
| **Climate & Environment** | • Weather forecasting (short→medium term)  <br>• Climate projection (long-term trends)  <br>• Event detection (extreme weather)  <br>• Environmental monitoring (air quality, water levels)                      | • Spatio-temporal FMs (global Transformers, GNNs) trained on historical simulations + observations  <br>• FMs as fast surrogates for physics-based models  <br>• Multimodal (satellite imagery + sensor TS) for integrated Earth monitoring                | *Pangu-Weather* – global weather Transformer (Huawei 2023)  <br>*Graph-Climate-FM* – GNN pre-trained on climate networks (2024)  <br>*STFM Survey* – tutorial on spatio-temporal FMs                                         |
| **Industry & IoT**        | • Predictive maintenance (sensor forecasting & fault detection)  <br>• Quality control (process-data anomalies)  <br>• Smart-grid/IoT monitoring (load, traffic etc.)                                            | • Large self-supervised models of sensor data for fault prediction  <br>• Cross-modal use of vision/audio FMs for sensor anomalies  <br>• Multi-task FMs spanning different equipment or facilities                                                        | *ITF-TAD* – image-FM-driven sensor anomaly detection  <br>*FactoryFM* (2025) – foundation model of manufacturing sensor streams  <br>*TrafficFM* – universal traffic-forecasting model pre-trained on city-wide networks |



---


### Table 2. Common Datasets and Evaluation Setups for Time Series Foundation Models

| **Task**                             | **Example Datasets**| **Evaluation Setup** | **Metrics**|
|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Forecasting**                      | • **M4** (100 k series, mixed domains) <br>• **ETT** family (electricity, long sequences) <br>• **Weather** sets – ETTh/ETTm, Wind, Solar <br>• **Traffic** – PEMS, METR-LA <br>• **Finance** – stock indices, FX rates <br>• **WeatherBench** (climate re-analysis) | • **Zero-shot transfer**: train on one corpus (or unlabeled data), test on a new domain <br>• **Few-shot fine-tune** on the target domain <br>• **Multi-horizon** evaluation (short vs. long) <br>• Compare zero / few / full-shot performance (à la FoundTS)                                                                                                                                    | MSE, MAE, RMSE, MAPE / sMAPE; sometimes MASE or OWA (for M4/M5-style comparisons); interval-forecast coverage for probabilistic models.                                                                                                                                                                                                                                  |
| **Classification**                   | • **UCR/UEA** archive (ECG200, Heartbeat, Cricket …) <br>• **PTB-XL** (labeled ECG) <br>• **TUH EEG** (normal vs. abnormal) <br>• **Activity** – WISDM, PAMAP <br>• Speech-TS (spoken-word audio)                                                          | • Standard train/test splits (archive-provided or k-fold) <br>• **Cross-dataset generalization**: train on one set, test on a related set (foundation-model check) <br>• Compare frozen representations vs. fine-tuned heads to gauge feature quality                                                                                                                                            | Accuracy, F1; AUROC (binary tasks), Calibration Error (ECE) when reported; inference latency for online cases.                                                                                                                                                                                                                                                            |
| **Anomaly Detection**                | • **NAB** benchmark (multi-domain) <br>• **Yahoo Webscope A1** <br>• **NASA SMAP & MSL** spacecraft sensors <br>• **SWaT / WADI** industrial-control cyber-attacks <br>• ECG anomaly series (arrhythmia)                                                 | • **Unsupervised** (learn on normal only, then threshold) or **semi-supervised** (few anomalies) <br>• Continuous-stream testing with labeled anomaly windows <br>• **Cross-machine/domain** transfer (train on one asset, test on another)                                                                                                                                                | Precision, Recall, F1 (point-adjusted); PR-AUC or ROC-AUC for anomaly scores; Mean Time to Detection (MTTD) for latency-sensitive studies.                                                                                                                                                                                                                                |
| **Representation Learning / Other**  | Massive unlabeled corpora: <br>• ~200 M-series compilation (UCR + web) used by **GTT** <br>• **PhysioBank** waveform archives <br>• Large industrial sensor dumps <br>*(downstream evaluation uses standard datasets above)*                               | • **Transfer tests**: freeze encoder, train a linear head on each downstream dataset <br>• **Multi-task fine-tune** on small slices of many tasks, report overall lift <br>• **Generative quality**: compare synthetic vs. real series (if FM is generative)                                                                                                                                | Task-specific downstream scores (accuracy, MSE, …); average rank / aggregated score across tasks; for generation: visual fidelity or distribution distance (e.g., Fréchet TS distance); efficiency stats (training time, parameter count, memory).                                                                                                                       |
